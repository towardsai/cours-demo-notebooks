{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "\n",
    "import instructor\n",
    "import logfire\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "logfire.configure()\n",
    "\n",
    "\n",
    "client = AsyncOpenAI(max_retries=3)\n",
    "\n",
    "logfire.instrument_openai(client)\n",
    "client = instructor.from_openai(client)\n",
    "\n",
    "\n",
    "INPUT_OUTPUT_PAIRS_FILE_PATH = \"test_queries.jsonl\"\n",
    "EVALUATION_RESULTS_FILE_PATH = \"task_evaluation_results.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydantic Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationReponse(BaseModel):\n",
    "    chain_of_thought: str = Field(\n",
    "        description=\"Comment vas-tu évaluer la réponse générée ?\"\n",
    "    )\n",
    "    jugement: bool = Field(\n",
    "        description=\"En utilisant le raisonnement ci-dessus, indique True si la réponse correspond à la réponse de référence (ground truth).\"\n",
    "    )\n",
    "    explication: str = Field(\n",
    "        description=\"Explique ici le raisonnement derrière ton évaluation. Quels sont les nombres qui ne sont pas pareils? Mentionne les nombres dans la référance et la réponse générée\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_file(jsonl_file_path):\n",
    "    data = []\n",
    "    with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "jsonl_file_path = INPUT_OUTPUT_PAIRS_FILE_PATH\n",
    "data = load_jsonl_file(jsonl_file_path)\n",
    "\n",
    "\n",
    "async def verify_classification(idx, item, client, semaphore):\n",
    "    async with semaphore:\n",
    "        query = item[\"query\"]\n",
    "        ground_truth = item[\"ground_truth\"]\n",
    "        generated_output = item[\"generated_output\"]\n",
    "        with logfire.span(f\"Question {idx}: {query}\"):\n",
    "            try:\n",
    "                verification_result: ValidationReponse = (\n",
    "                    await client.chat.completions.create(\n",
    "                        model=\"gpt-4o\",\n",
    "                        response_model=ValidationReponse,\n",
    "                        messages=[\n",
    "                            {\n",
    "                                \"role\": \"system\",\n",
    "                                \"content\": (\n",
    "                                    \"La tâche consiste à vérifier si la réponse générée correspond à la réponse de référence (ground truth).\"\n",
    "                                    \"Tant que la réponse générée contient la ou les bonnes valeurs présentes dans la réponse de référence, la réponse est considérée comme correcte.\"\n",
    "                                    \"Les nombres mentionnés dans la réponse générée doivent être exactement identiques à ceux de la réponse de référence.\"\n",
    "                                    \"Le format de la réponse n'est pas à considérer, tant que les valeurs sont identiques.\"\n",
    "                                    \"Mentionne les nombres de la réponse de référance et générée dans ta résponse\"\n",
    "                                ),\n",
    "                            },\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": (\n",
    "                                    f\"Voici la question originale :\\n{query}\\n\\n\"\n",
    "                                    f\"Maintenant, vérifie si la réponse générée est correcte et répond à la question.\\n\"\n",
    "                                    f\"Voici la réponse de référance :\\n{ground_truth}\\n\\n\"\n",
    "                                    f\"et voici la réponse générée :\\n{generated_output}\\n\\n\"\n",
    "                                ),\n",
    "                            },\n",
    "                        ],\n",
    "                        max_tokens=8000,\n",
    "                        temperature=0,\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logfire.error(f\"Error processing input example {idx}: {e}\")\n",
    "\n",
    "            return idx, verification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    semaphore = asyncio.Semaphore(100)\n",
    "    tasks = [\n",
    "        verify_classification(idx, item, client, semaphore)\n",
    "        for idx, item in enumerate(data, start=1)\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks)):\n",
    "        idx, verification = await future\n",
    "        results.append((idx, verification))\n",
    "\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    verification_file_path = EVALUATION_RESULTS_FILE_PATH\n",
    "    with open(verification_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, verification in results:\n",
    "            json_line = json.dumps(\n",
    "                {\n",
    "                    \"index\": idx,\n",
    "                    \"evaluation\": verification.model_dump(),\n",
    "                },\n",
    "                ensure_ascii=False,\n",
    "            )\n",
    "            f.write(json_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load evaluation results from the JSONL file\n",
    "def load_evaluation_results(jsonl_file_path):\n",
    "    evaluation_results = []\n",
    "    with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data = json.loads(line)\n",
    "                evaluation_results.append(data)\n",
    "    return evaluation_results\n",
    "\n",
    "\n",
    "# Load the evaluation results\n",
    "evaluation_file_path = EVALUATION_RESULTS_FILE_PATH\n",
    "evaluation_results = load_evaluation_results(evaluation_file_path)\n",
    "\n",
    "\n",
    "# Count true/false judgments and calculate accuracy\n",
    "def calculate_accuracy(evaluation_results):\n",
    "    total_evaluations = len(evaluation_results)\n",
    "    true_count = sum(\n",
    "        1\n",
    "        for result in evaluation_results\n",
    "        if result.get(\"evaluation\", {}).get(\"jugement\") == True\n",
    "    )\n",
    "    false_count = total_evaluations - true_count\n",
    "\n",
    "    accuracy = true_count / total_evaluations if total_evaluations > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"total_evaluations\": total_evaluations,\n",
    "        \"true_count\": true_count,\n",
    "        \"false_count\": false_count,\n",
    "        \"accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate and display the accuracy metrics\n",
    "accuracy_metrics = calculate_accuracy(evaluation_results)\n",
    "print(f\"Total evaluations: {accuracy_metrics['total_evaluations']}\")\n",
    "print(f\"Correct answers (True): {accuracy_metrics['true_count']}\")\n",
    "print(f\"Incorrect answers (False): {accuracy_metrics['false_count']}\")\n",
    "print(f\"Accuracy: {accuracy_metrics['accuracy']:.2%}\")\n",
    "\n",
    "\n",
    "# Display the questions the system got wrong\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INCORRECT ANSWERS (jugement=False)\".center(80))\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "incorrect_answers = [\n",
    "    result\n",
    "    for result in evaluation_results\n",
    "    if result.get(\"evaluation\", {}).get(\"jugement\") == False\n",
    "]\n",
    "\n",
    "if incorrect_answers:\n",
    "    for i, result in enumerate(incorrect_answers, 1):\n",
    "        index = result.get(\"index\", \"Unknown\")\n",
    "        explication = result.get(\"evaluation\", {}).get(\n",
    "            \"explication\", \"No reasoning provided\"\n",
    "        )\n",
    "\n",
    "        print(f\"❌ INCORRECT ANSWER #{i} (Index: {index})\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"explication:\")\n",
    "        print(f\"{explication}\")\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "else:\n",
    "    print(\"No incorrect answers found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
